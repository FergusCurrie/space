{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c35d51-f2f1-455a-aae4-c2f376d00da8",
   "metadata": {},
   "source": [
    "# LLAMAv2\n",
    "\n",
    "Setup:\n",
    "https://huggingface.co/blog/llama2\n",
    "\n",
    "\n",
    "Model Options: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e88b9ff-127f-494b-989f-d891defe0ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asttokens==2.2.1\n",
      "backcall==0.2.0\n",
      "blis==0.7.9\n",
      "catalogue==2.0.8\n",
      "certifi==2023.5.7\n",
      "charset-normalizer==3.2.0\n",
      "click==8.1.6\n",
      "cmake==3.27.0\n",
      "comm==0.1.3\n",
      "confection==0.1.0\n",
      "cymem==2.0.7\n",
      "debugpy==1.6.7\n",
      "decorator==5.1.1\n",
      "executing==1.2.0\n",
      "Faker==19.1.0\n",
      "filelock==3.12.2\n",
      "fsspec==2023.6.0\n",
      "huggingface-hub==0.16.4\n",
      "idna==3.4\n",
      "install==1.3.5\n",
      "ipykernel==6.24.0\n",
      "ipython==8.14.0\n",
      "jedi==0.18.2\n",
      "Jinja2==3.1.2\n",
      "jupyter_client==8.3.0\n",
      "jupyter_core==5.3.1\n",
      "langcodes==3.3.0\n",
      "lit==16.0.6\n",
      "MarkupSafe==2.1.3\n",
      "matplotlib-inline==0.1.6\n",
      "mpmath==1.3.0\n",
      "murmurhash==1.0.9\n",
      "mypy==1.4.1\n",
      "mypy-extensions==1.0.0\n",
      "neo4j==5.10.0\n",
      "nest-asyncio==1.5.6\n",
      "networkx==3.1\n",
      "numpy==1.25.1\n",
      "nvidia-cublas-cu11==11.10.3.66\n",
      "nvidia-cuda-cupti-cu11==11.7.101\n",
      "nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "nvidia-cuda-runtime-cu11==11.7.99\n",
      "nvidia-cudnn-cu11==8.5.0.96\n",
      "nvidia-cufft-cu11==10.9.0.58\n",
      "nvidia-curand-cu11==10.2.10.91\n",
      "nvidia-cusolver-cu11==11.4.0.1\n",
      "nvidia-cusparse-cu11==11.7.4.91\n",
      "nvidia-nccl-cu11==2.14.3\n",
      "nvidia-nvtx-cu11==11.7.91\n",
      "packaging==23.1\n",
      "pandas==2.0.3\n",
      "parso==0.8.3\n",
      "pathy==0.10.2\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "platformdirs==3.9.1\n",
      "preshed==3.0.8\n",
      "prompt-toolkit==3.0.39\n",
      "psutil==5.9.5\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pydantic==1.10.11\n",
      "Pygments==2.15.1\n",
      "python-dateutil==2.8.2\n",
      "pytz==2023.3\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.0\n",
      "regex==2023.6.3\n",
      "requests==2.31.0\n",
      "safetensors==0.3.1\n",
      "six==1.16.0\n",
      "smart-open==6.3.0\n",
      "spacy==3.6.0\n",
      "spacy-legacy==3.0.12\n",
      "spacy-loggers==1.0.4\n",
      "srsly==2.4.7\n",
      "stack-data==0.6.2\n",
      "sympy==1.12\n",
      "thinc==8.1.10\n",
      "tokenizers==0.13.3\n",
      "tomli==2.0.1\n",
      "torch==2.0.1\n",
      "tornado==6.3.2\n",
      "tqdm==4.65.0\n",
      "traitlets==5.9.0\n",
      "transformers @ git+https://github.com/huggingface/transformers@6112b1c6442aaf7affd2b0676a1cd4eee30c45cf\n",
      "triton==2.0.0\n",
      "typer==0.9.0\n",
      "typing_extensions==4.7.1\n",
      "tzdata==2023.3\n",
      "urllib3==2.0.3\n",
      "wasabi==1.1.2\n",
      "wcwidth==0.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d33cb8d-44d4-4bea-a6a8-e8259c3bf550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fergus/repos/space/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee26750e-07be-4678-90a0-5d7d797a2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "480fe275-7b8c-448c-af48-e936c91b018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a8dc5fc-5281-4e58-a8fe-9c3602becf7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/fergus/repos/transformers/hf_llama\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model)\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n",
      "File \u001b[0;32m~/repos/space/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1058\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1058\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/space/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:1046\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1044\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"/home/fergus/repos/transformers/hf_llama\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model)\n",
    "model = LlamaForCausalLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c76598-0508-464f-85d7-f34413aa9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacevenv",
   "language": "python",
   "name": "spacevenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d26f47-c43b-497b-82e5-77eedadf2194",
   "metadata": {},
   "source": [
    "# Parse RL markdown file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2479b861-b188-486f-a1ed-cb50eb496cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UnorderedProperty:\n",
    "    name: str\n",
    "    value: set[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OrderedProperty:\n",
    "    name: str\n",
    "    value: list[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Definition:\n",
    "    value1: str\n",
    "    value2: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cfff712-6894-4f47-9d1b-26883ea76094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Structure Learning = Learning structure of bayesian network.',\n",
       " 'Two way of exact policy evaluation:',\n",
       " '- system of equations',\n",
       " '- lookahead',\n",
       " 'Value function = expected return from a state',\n",
       " 'Value function policy = extract policy from value function',\n",
       " 'Exact ways to get policy from value function:',\n",
       " '- policy iteration',\n",
       " '- value iteration',\n",
       " '- linear program',\n",
       " '- greedy',\n",
       " 'Value iteration = fixed point update of value function with bellman equation',\n",
       " 'General three steps of RL:',\n",
       " '1. Sample trajectories',\n",
       " '2. Evaluate Policy',\n",
       " '3. Improve Policy',\n",
       " 'Policy gradient algorithm:',\n",
       " '1. Parameterise policy with nn : $\\\\pi\\\\_\\\\theta $',\n",
       " '2. $\\\\nabla_\\\\theta J(\\\\theta)$',\n",
       " '3. $\\\\theta \\\\leftarrow \\\\theta + \\\\alpha \\\\nabla_\\\\theta J(\\\\theta)$',\n",
       " 'policy gradient = $\\\\nabla_\\\\theta J(\\\\theta) = \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi\\\\_\\\\theta}[\\\\sum_{t=0}^T \\\\nabla_\\\\theta \\\\log \\\\pi\\\\_\\\\theta(a_t|s_t) R(\\\\tau)]$',\n",
       " 'the relationship between policy gradient and mle is policy gradient is mle of the log likelihood of the trajectory \\\\* reward of trajectory',\n",
       " 'four main types of RL algorithms:',\n",
       " '- policy gradient',\n",
       " '- actor critic',\n",
       " '- model based',\n",
       " '- value based',\n",
       " 'trajectory = sequence of state action pairs',\n",
       " 'temporal difference equation = $r(s_t, a_t) + U(s_{t+1}) - U(s_t)$',\n",
       " 'single sample estimator = dataset of $\\\\{ (s_t, \\\\sum^T_k=t{r_t}) \\\\}$',\n",
       " 'the downside of single sample estimaor is that it has high variance because it only uses one sample',\n",
       " 'bootstrap in actor critic = dataset of $\\\\{ (s_t, r(s_t, a_t) + U(s_{t+1})) \\\\}$',\n",
       " 'causality trick = reduce variance by only weighing states by future rewards',\n",
       " 'causality trick is parameterised q-function',\n",
       " 'baseline subtraction = reduce variance by subtracting a baseline from the reward',\n",
       " 'two ways of reducing variance in policy gradient method:',\n",
       " '- causality trick',\n",
       " '- baseline subtraction',\n",
       " 'action value function aka q-function',\n",
       " 'q-function = reward from being in state, taking action a, then following policy',\n",
       " 'advtange function = $A(s,a) = Q(s,a) - U(a)$',\n",
       " 'refinrocement learning objective = $\\\\argmax_\\\\theta E_{\\\\tau} [R(\\\\tau)]$']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_markdown_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        file = [x for x in f.read().split(\"\\n\\n\") if x != \"\"]\n",
    "        f = []\n",
    "        for line in file:\n",
    "            for x in line.split('\\n'):\n",
    "                if x != '':\n",
    "                    f.append(x)\n",
    "        return f \n",
    "    return file \n",
    "    \n",
    "text = read_markdown_file(\"../data/facts/rl.md\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6eb67b18-f584-468d-952d-a0cc60ce484e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Definition(value1='Structure Learning ', value2='Learning structure of bayesian network.'),\n",
       " UnorderedProperty(name='Two way of exact policy evaluation', value={' system of equations', ' lookahead'}),\n",
       " Definition(value1='Value function ', value2='expected return from a state'),\n",
       " Definition(value1='Value function policy ', value2='extract policy from value function'),\n",
       " UnorderedProperty(name='Exact ways to get policy from value function', value={' greedy', ' linear program', ' value iteration', ' policy iteration'}),\n",
       " Definition(value1='Value iteration ', value2='fixed point update of value function with bellman equation'),\n",
       " UnorderedProperty(name='General three steps of RL', value=set()),\n",
       " UnorderedProperty(name='Policy gradient algorithm', value=set()),\n",
       " UnorderedProperty(name='1. Parameterise policy with nn  $\\\\pi\\\\_\\\\theta $', value=set()),\n",
       " Definition(value1='policy gradient ', value2='$\\\\nabla_\\\\theta J(\\\\theta) = \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi\\\\_\\\\theta}[\\\\sum_{t=0}^T \\\\nabla_\\\\theta \\\\log \\\\pi\\\\_\\\\theta(a_t|s_t) R(\\\\tau)]$'),\n",
       " UnorderedProperty(name='four main types of RL algorithms', value={' policy gradient', ' actor critic', ' model based', ' value based'}),\n",
       " Definition(value1='trajectory ', value2='sequence of state action pairs'),\n",
       " Definition(value1='temporal difference equation ', value2='$r(s_t, a_t) + U(s_{t+1}) - U(s_t)$'),\n",
       " Definition(value1='single sample estimator ', value2='dataset of $\\\\{ (s_t, \\\\sum^T_k=t{r_t}) \\\\}$'),\n",
       " Definition(value1='bootstrap in actor critic ', value2='dataset of $\\\\{ (s_t, r(s_t, a_t) + U(s_{t+1})) \\\\}$'),\n",
       " Definition(value1='causality trick ', value2='reduce variance by only weighing states by future rewards'),\n",
       " Definition(value1='baseline subtraction ', value2='reduce variance by subtracting a baseline from the reward'),\n",
       " UnorderedProperty(name='two ways of reducing variance in policy gradient method', value={' causality trick', ' baseline subtraction'}),\n",
       " Definition(value1='q-function ', value2='reward from being in state, taking action a, then following policy'),\n",
       " Definition(value1='advtange function ', value2='$A(s,a) = Q(s,a) - U(a)$'),\n",
       " Definition(value1='refinrocement learning objective ', value2='$\\\\argmax_\\\\theta E_{\\\\tau} [R(\\\\tau)]$')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def check_string_starts_numeric(my_string: str) -> bool:\n",
    "    if re.match(r\"^\\d\", my_string):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def split_first_occurance(my_string, pattern) -> list[str]:\n",
    "    split = my_string.split(pattern)\n",
    "    i = my_string.index(pattern)\n",
    "    result = [split[0], my_string[i+1:].strip()]\n",
    "    return result\n",
    "\n",
    "filename = \"../data/facts/rl.md\"\n",
    "file: str = read_markdown_file(filename)\n",
    "results = []\n",
    "for i in range(len(file)):\n",
    "    line = file[i]\n",
    "    if '=' in line:\n",
    "        key, value = split_first_occurance(line, \"=\")\n",
    "        results.append(Definition(value1=key, value2=value))\n",
    "    if ':' in line:\n",
    "        j = 1\n",
    "        collection = True\n",
    "        unordered_list = set()\n",
    "        ordered_list = []\n",
    "        while collection:\n",
    "            if i+j < len(file):\n",
    "                next_line = file[i+j]\n",
    "                if next_line.startswith('-'):\n",
    "                    unordered_list.add(next_line.replace('-', ''))\n",
    "                elif check_string_starts_numeric(next_line):\n",
    "                    ordered_list.append(next_line.replace('-', ''))\n",
    "                else:\n",
    "                    j -= 1\n",
    "                    break\n",
    "                j += 1\n",
    "            else:\n",
    "                break\n",
    "        i += j\n",
    "        if unordered_list != []:\n",
    "            results.append(UnorderedProperty(name=line.replace(':',\"\"), value=unordered_list))\n",
    "        elif ordered_list != []:\n",
    "            results.append(OrderedProperty(name=line.replace(':',\"\"), value=ordered_list))\n",
    "results     \n",
    "    \n",
    "        \n",
    "        \n",
    "                    \n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe013e9-45a6-4306-8a87-19f6952c1033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11bf0ad-a522-4ef3-9d50-0f52ceacf9f8",
   "metadata": {},
   "source": [
    "# Notebook to test pipeline from markdown file into neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7862ae1-4cdb-48a5-b430-1f9e4372f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.deepreload import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6767f74-42e0-4f9b-97e4-92e4f6ae3579",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## First deal with imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ff11c9-7c1a-4803-855e-2c31184d8d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if \"backend\" not in os.listdir():\n",
    "    os.chdir(\"..\")\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd() + \"/.venv/lib/python3.10/site-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e957f5-1be8-416c-ac01-4ae65e849c2f",
   "metadata": {},
   "source": [
    "## Parse definitions from markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "875ac063-245d-4a41-9124-b30f1a7758d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.custom_dataclasses import Definition, OrderedProperty, UnorderedProperty\n",
    "from backend.parser import parse_markdown_file \n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"data/facts/rl.md\")\n",
    "parsed_data = parse_markdown_file(path)\n",
    "definitions_parsed_data = [data for data in parsed_data if type(data) == Definition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f516e8b-9c66-404f-924b-b83450a46c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Definition(value1='Structure Learning ', value2='Learning structure of bayesian network.'), Definition(value1='Value function ', value2='expected return from a state'), Definition(value1='Value function policy ', value2='extract policy from value function'), Definition(value1='Value iteration ', value2='fixed point update of value function with bellman equation'), Definition(value1='policy gradient ', value2='$\\\\nabla_\\\\theta J(\\\\theta) = \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi\\\\_\\\\theta}[\\\\sum_{t=0}^T \\\\nabla_\\\\theta \\\\log \\\\pi\\\\_\\\\theta(a_t|s_t) R(\\\\tau)]$'), Definition(value1='trajectory ', value2='sequence of state action pairs'), Definition(value1='temporal difference equation ', value2='$r(s_t, a_t) + U(s_{t+1}) - U(s_t)$'), Definition(value1='single sample estimator ', value2='dataset of $\\\\{ (s_t, \\\\sum^T_k=t{r_t}) \\\\}$'), Definition(value1='bootstrap in actor critic ', value2='dataset of $\\\\{ (s_t, r(s_t, a_t) + U(s_{t+1})) \\\\}$'), Definition(value1='causality trick ', value2='reduce variance by only weighing states by future rewards'), Definition(value1='baseline subtraction ', value2='reduce variance by subtracting a baseline from the reward'), Definition(value1='q-function ', value2='reward from being in state, taking action a, then following policy'), Definition(value1='advtange function ', value2='$A(s,a) = Q(s,a) - U(a)$'), Definition(value1='refinrocement learning objective ', value2='$\\\\argmax_\\\\theta E_{\\\\tau} [R(\\\\tau)]$')]\n"
     ]
    }
   ],
   "source": [
    "print(definitions_parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb45bffb-4b8f-4700-a765-29060f5e7379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedProperty(name='General three steps of RL', value=['1. Sample trajectories', '2. Evaluate Policy', '3. Improve Policy']),\n",
       " OrderedProperty(name='Policy gradient algorithm', value=['1. Parameterise policy with nn : $\\\\pi\\\\_\\\\theta $', '2. $\\\\nabla_\\\\theta J(\\\\theta)$', '3. $\\\\theta \\\\leftarrow \\\\theta + \\\\alpha \\\\nabla_\\\\theta J(\\\\theta)$'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_property_parsed_data = [data for data in parsed_data if type(data) == OrderedProperty]\n",
    "ordered_property_parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "589c91c8-6501-4702-84e9-1ccfeba24fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UnorderedProperty(name='Two way of exact policy evaluation', value={' lookahead', ' system of equations'}),\n",
       " UnorderedProperty(name='Exact ways to get policy from value function', value={' linear program', ' greedy', ' policy iteration', ' value iteration'}),\n",
       " UnorderedProperty(name='four main types of RL algorithms', value={' actor critic', ' value based', ' model based', ' policy gradient'}),\n",
       " UnorderedProperty(name='two ways of reducing variance in policy gradient method', value={' causality trick', ' baseline subtraction'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unordered_property_parsed_data = [data for data in parsed_data if type(data) == UnorderedProperty]\n",
    "unordered_property_parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38100264-67bb-4f3c-a350-ba1ca0c0febd",
   "metadata": {},
   "source": [
    "## Load defintions into neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f10a196-1cb6-4c64-9e2b-9bc8290e5ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.neo_connector import NeoConnector\n",
    "neo = NeoConnector()\n",
    "neo.fill_defintions(definitions_parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2220184e-2e07-4a65-ae92-0b8354a2bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo.fill_unordered_properties(unordered_property_parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42bffb9b-e521-4097-96d2-cf27ef540758",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo.fill_ordered_properties(ordered_property_parsed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "991841cd-f4df-4e7c-b3cb-5c969deaa4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedProperty(name='General three steps of RL', value=['1. Sample trajectories', '2. Evaluate Policy', '3. Improve Policy']),\n",
       " OrderedProperty(name='Policy gradient algorithm', value=['1. Parameterise policy with nn : $\\\\pi\\\\_\\\\theta $', '2. $\\\\nabla_\\\\theta J(\\\\theta)$', '3. $\\\\theta \\\\leftarrow \\\\theta + \\\\alpha \\\\nabla_\\\\theta J(\\\\theta)$'])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_property_parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb29565-50f6-469b-b55e-e94cef8fd03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

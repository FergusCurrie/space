{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d26f47-c43b-497b-82e5-77eedadf2194",
   "metadata": {},
   "source": [
    "# Parse RL markdown file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f34c0cd6-b22e-4a16-b955-189b9792953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.deepreload import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8b59b4-d8e8-42c6-93c5-af688a135d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if \"backend\" not in os.listdir():\n",
    "    os.chdir(\"..\")\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd() + \"/.venv/lib/python3.10/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5a61057-4787-4c1d-800e-0ff92d65379b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Definition(value1='Structure Learning ', value2='Learning structure of bayesian network.'),\n",
       " UnorderedProperty(name='Two way of exact policy evaluation', value={' lookahead', ' system of equations'}),\n",
       " Definition(value1='Value function ', value2='expected return from a state'),\n",
       " Definition(value1='Value function policy ', value2='extract policy from value function'),\n",
       " UnorderedProperty(name='Exact ways to get policy from value function', value={' policy iteration', ' linear program', ' greedy', ' value iteration'}),\n",
       " Definition(value1='Value iteration ', value2='fixed point update of value function with bellman equation'),\n",
       " OrderedProperty(name='General three steps of RL', value=['1. Sample trajectories', '2. Evaluate Policy', '3. Improve Policy']),\n",
       " OrderedProperty(name='Policy gradient algorithm', value=['1. Parameterise policy with nn : $\\\\pi\\\\_\\\\theta $', '2. $\\\\nabla_\\\\theta J(\\\\theta)$', '3. $\\\\theta \\\\leftarrow \\\\theta + \\\\alpha \\\\nabla_\\\\theta J(\\\\theta)$']),\n",
       " Definition(value1='policy gradient ', value2='$\\\\nabla_\\\\theta J(\\\\theta) = \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi\\\\_\\\\theta}[\\\\sum_{t=0}^T \\\\nabla_\\\\theta \\\\log \\\\pi\\\\_\\\\theta(a_t|s_t) R(\\\\tau)]$'),\n",
       " UnorderedProperty(name='four main types of RL algorithms', value={' value based', ' model based', ' policy gradient', ' actor critic'}),\n",
       " Definition(value1='trajectory ', value2='sequence of state action pairs'),\n",
       " Definition(value1='temporal difference equation ', value2='$r(s_t, a_t) + U(s_{t+1}) - U(s_t)$'),\n",
       " Definition(value1='single sample estimator ', value2='dataset of $\\\\{ (s_t, \\\\sum^T_k=t{r_t}) \\\\}$'),\n",
       " Definition(value1='bootstrap in actor critic ', value2='dataset of $\\\\{ (s_t, r(s_t, a_t) + U(s_{t+1})) \\\\}$'),\n",
       " Definition(value1='causality trick ', value2='reduce variance by only weighing states by future rewards'),\n",
       " Definition(value1='baseline subtraction ', value2='reduce variance by subtracting a baseline from the reward'),\n",
       " UnorderedProperty(name='two ways of reducing variance in policy gradient method', value={' baseline subtraction', ' causality trick'}),\n",
       " Definition(value1='q-function ', value2='reward from being in state, taking action a, then following policy'),\n",
       " Definition(value1='advtange function ', value2='$A(s,a) = Q(s,a) - U(a)$'),\n",
       " Definition(value1='refinrocement learning objective ', value2='$\\\\argmax_\\\\theta E_{\\\\tau} [R(\\\\tau)]$')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from backend.parser import parse_markdown_file\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"data/facts/rl.md\")\n",
    "parsed_data = parse_markdown_file(path)\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d655b2-6d92-4471-9601-98a5133fbc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
